# GP-BLR

This repository covers the write-up and code for "Gaussian processes and equivalent Bayesian linear regressions" to address a gap we found in the machine learning literature on how to produce an equivalent Bayesian linear regression (BLR) given a kernel for a Gaussian process (GP). Studying BLR and GP side-by-side is critical for developing any Bayesian application, since these are among only a few models in  circulation that allow you to learn the parameters using explicit and deterministic update rules. In other words, if we want to put Bayesian models into production with calibrated uncertainty predictions, these two are the robust, reliable baselines from which all other approaches are compared. Moreover, because the computation of these models is deterministic, we avoid the horrible pitfalls of Markov chain Monte Carlo, and because they are exact, we avoid the pitfalls of variational inference. Both are actually parts of the same continuum between high featurization and large amounts of data. 

In addition, finding the equivalent Bayesian linear regression (BLR) to a Gaussian process (GP) can provide substantial benefits in modeling including (1) enormous efficiency savings with large amounts of data, (2) access to incremental (sequential or online) computation methods, (3) access to vetted approximation methods for efficiency (such as the Laplace approximation for regressors, not just classifiers), (4) easier interpretability, and (5) the ability to effortlessly to combine multiple models into one without using bespoke software libraries. However, while the three major textbooks on the subject (Bishop, Murphy, and Rasmussen) cover the subject extensively, it surprised me that they didn't provide a practical side-by-side comparison or recipe for converting the kernel used to generate a GP to an equivalent set of basis functions for a BLR. Moreover, we can look at Gaussian processes as an extension of Bayesian linear regression into the territory of small data, where the number of training points is less than the number of features, which is an important problem space for optimization procedures where evaluating a function may take a substantial amount of time. Bringing together the best of both worlds is enormously beneficial to anyone working within the optimization problem space.
